{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Transformer Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 512\n",
    "block_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''one head of self-attention'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores (\"AFFINITIES\")\n",
    "        wei = q @ k.transpose(-2, -1) * c**-0.5\n",
    "        wei = wei.masker_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        # perform the wighted aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## RWKV Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision = 4, suppress = True, linewidth = 200)\n",
    "import types\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载分词器\n",
    "tokenizer = Tokenizer.from_file('20B_tokenizer.json')\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "\n",
    "args.MODEL_NAME = 'RWKV-4-Pile-430M-20220808-8066'\n",
    "args.n_layer = 24\n",
    "args.n_embd = 1024\n",
    "\n",
    "context = \"\\n\"\n",
    "NUM_TRIALS = 3\n",
    "LENGTH_PER_TRIAL = 100\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 0.85\n",
    "\n",
    "class RWKV_RNN(torch.jit.ScriptModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.eval() # 将模型设置为评估模式，这样在模型中就不会使用dropout和batch normalization\n",
    "\n",
    "\n",
    "        w = torch.load(args.MODEL_NAME + '.pth', map_location = 'cpu')\n",
    "        for k in w.keys():\n",
    "            if '.time_' in k:\n",
    "                w[k] = w[k].squeeze()\n",
    "            if '.time_decay' in k:\n",
    "                w[k] = -torch.exp(w[k].float())\n",
    "            else:\n",
    "                w[k] = w[k].float()\n",
    "\n",
    "        self.w = types.SimpleNamespace()\n",
    "        self.w.blocks = {}\n",
    "        for k in w.keys():\n",
    "            parts = k.split('.')\n",
    "            last = parts.pop()\n",
    "            here = self.w\n",
    "            for p in parts:\n",
    "                if p.isdigit():\n",
    "                    p = int(p)\n",
    "                    if p not in here:\n",
    "                        here[p] = types.SimpleNamespace()\n",
    "                    here = here[p]\n",
    "                else:\n",
    "                    if not hasattr(here, p):\n",
    "                        setattr(here, p, types.SimpleNamespace())\n",
    "                    here = getattr(here, p)\n",
    "            setattr(here, last, w[k])\n",
    "\n",
    "    def layer_norm(self,x, w):\n",
    "        return F.layer_norm(x, (self.args.n_embd,), weight = w.weight, bias = w.bias)\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def channel_mixing(self, x, state, i:int, time_mix_k, time_mix_r, kw, vw, rw):\n",
    "        '''Channel mixing function for RWKV model'''\n",
    "        xk = x * time_mix_k + state[5*i+0] * (1 - time_mix_k)\n",
    "        xr = x * time_mix_r + state[5*i+0] * (1 - time_mix_r)\n",
    "        state[5*i+0] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = torch.square(torch.relu(kw @ xk)) # square relu, primer paper\n",
    "        return r * (vw @ k)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def time_mixing(self, x, state, i:int, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow):\n",
    "        '''Time mixing function for RWKV model'''\n",
    "        xk = x * time_mix_k + state[5*i+1] * (1 - time_mix_k)\n",
    "        xv = x * time_mix_v + state[5*i+1] * (1 - time_mix_v)\n",
    "        xr = x * time_mix_r + state[5*i+1] * (1 - time_mix_r)\n",
    "        state[5*i+1] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = kw @ xk\n",
    "        v = vw @ xv\n",
    "        \n",
    "        aa = state[5*i+2]\n",
    "        bb = state[5*i+3]\n",
    "        pp = state[5*i+4]\n",
    "        ww = time_first + k\n",
    "        qq = torch.maximum(pp, ww)\n",
    "        e1 = torch.exp(pp - qq)\n",
    "        e2 = torch.exp(ww - qq)\n",
    "        a = e1 * aa + e2 * v\n",
    "        b = e1 * bb + e2\n",
    "        wkv = a / b\n",
    "        ww = pp + time_decay\n",
    "        qq = torch.maximum(ww, k)\n",
    "        e1 = torch.exp(ww - qq)\n",
    "        e2 = torch.exp(k - qq)\n",
    "        state[5*i+2] = e1 * aa + e2 * v\n",
    "        state[5*i+3] = e1 * bb + e2\n",
    "        state[5*i+4] = qq\n",
    "        return ow @ (r * wkv)\n",
    "\n",
    "    def forward(self, token, state):\n",
    "        '''前向传播函数，输入token和state，输出token的下一个token和更新后的state'''\n",
    "        with torch.no_grad():\n",
    "            if state == None:\n",
    "                state = torch.zeros(self.args.n_layer * 5, self.args.n_embd)\n",
    "                for i in range(self.args.n_layer): state[5*i+4] = -1e30 # -infinity\n",
    "            \n",
    "            x = self.w.emb.weight[token]\n",
    "            x = self.layer_norm(x, self.w.blocks[0].ln0)\n",
    "            for i in range(self.args.n_layer):\n",
    "                att = self.w.blocks[i].att\n",
    "                x = x + self.time_mixing(self.layer_norm(x, self.w.blocks[i].ln1), state, i, \n",
    "                    att.time_mix_k, att.time_mix_v, att.time_mix_r, att.time_first, att.time_decay, \n",
    "                    att.key.weight, att.value.weight, att.receptance.weight, att.output.weight)\n",
    "                ffn = self.w.blocks[i].ffn\n",
    "                x = x + self.channel_mixing(self.layer_norm(x, self.w.blocks[i].ln2), state, i, \n",
    "                    ffn.time_mix_k, ffn.time_mix_r, \n",
    "                    ffn.key.weight, ffn.value.weight, ffn.receptance.weight)\n",
    "            \n",
    "            x = self.w.head.weight @ self.layer_norm(x, self.w.ln_out)\n",
    "            return x.float(), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "采样函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_logits(out, temperature = 1.0, top_p = 0.8):\n",
    "    probs = F.softmax(out, dim = -1).numpy()\n",
    "\n",
    "    sorted_probs = np.sort(probs)[::-1]\n",
    "    cumulative_probs = np.cumsum(sorted_probs)\n",
    "    cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n",
    "    probs[probs < cutoff] = 0\n",
    "    if temperature != 1.0:\n",
    "        probs = probs.pow(1.0 / temperature)\n",
    "    probs = probs / np.sum(probs)\n",
    "\n",
    "    out = np.random.choice(a = len(probs), p = probs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "文本生成流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using CPU to load RWKV-4-Pile-430M-20220808-8066 ...\n",
      "\n",
      "Preprocessing context (slow version. see v2/rwkv/model.py for fast version)\n",
      "\n",
      "\n",
      "--[ Trial 0]----------------- \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. According to the scientists, the dragons are the oldest living creatures in the entire world.\n",
      "\n",
      "The strange, mysterious, and unique dragons have been living in this remote and inaccessible valley for thousands of years. This isolated valley was believed to be the world’s oldest active volcano, and the ancient Buddhist sage said that it should be removed from the endangered species list.\n",
      "\n",
      "The researchers said that many previous scientific studies failed to agree that the dragons live in Tibet. The researchers also suspected that the dragons\n",
      "\n",
      "--[ Trial 1]----------------- \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "However, their strange abilities may have come from an ancient dragon's magical sword, apparently a fearsome weapon.\n",
      "\n",
      "It's believed that the dragons are in the early stages of evolution and could have learned the rudiments of communication. However, they may not have noticed that, as the reptile learns, it can eat meat.\n",
      "\n",
      "They came from a place that's closer to home, and were deemed as the leaders of the mountain-dwelling nomads, according to The\n",
      "\n",
      "--[ Trial 2]----------------- \n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "“By comparing dragon dialect with radio interviews, we have uncovered a cultural heritage which no one had imagined existed before,” said Pasha Singh, an associate professor at Dalhousie University in Halifax, Nova Scotia.\n",
      "\n",
      "“The dragons lived in a totally isolated valley with a few thousand people. In the hills and in the forest, people had no idea of their surroundings, and were completely unfamiliar with each other,” Singh said.\n",
      "\n",
      "Researchers captured hundreds of dragons and many\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 打印使用CPU 加载模型的信息， 其中 args.MODEL_NAME 是模型名称\n",
    "print(f'\\nUsing CPU to load {args.MODEL_NAME} ...')\n",
    "model = RWKV_RNN(args)\n",
    "\n",
    "print(f'\\nPreprocessing context (slow version. see v2/rwkv/model.py for fast version)')\n",
    "init_state = None\n",
    "for token in tokenizer.encode(context).ids:\n",
    "    init_out, init_state = model.forward(token, init_state)\n",
    "\n",
    "for TRIAL in range(NUM_TRIALS):\n",
    "    print(f'\\n\\n--[ Trial {TRIAL}]-----------------', context, end = \"\")\n",
    "    all_tokens = []\n",
    "    out_last = 0\n",
    "    out, state = init_out.clone(), init_state.clone()\n",
    "    for i in range(LENGTH_PER_TRIAL):\n",
    "        token = sample_logits(out, TEMPERATURE, TOP_P)\n",
    "        all_tokens += [token]\n",
    "        tmp = tokenizer.decode(all_tokens[out_last:])\n",
    "        if '\\nfffd' not in tmp:\n",
    "            print(tmp, end = \"\", flush = True)\n",
    "            out_last = i + 1\n",
    "        out, state = model.forward(token,state)\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
